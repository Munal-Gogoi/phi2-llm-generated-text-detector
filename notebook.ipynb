{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12256616,"sourceType":"datasetVersion","datasetId":7723158},{"sourceId":12304683,"sourceType":"datasetVersion","datasetId":7755870}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers accelerate bitsandbytes peft pandas pyarrow\nprint(\"Libraries installed!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:14:29.632674Z","iopub.execute_input":"2025-07-01T19:14:29.633256Z","iopub.status.idle":"2025-07-01T19:14:32.877700Z","shell.execute_reply.started":"2025-07-01T19:14:29.633229Z","shell.execute_reply":"2025-07-01T19:14:32.876739Z"}},"outputs":[{"name":"stdout","text":"Libraries installed!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    DataCollatorForLanguageModeling\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom torch.utils.data import Dataset, DataLoader\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:14:40.192885Z","iopub.execute_input":"2025-07-01T19:14:40.193191Z","iopub.status.idle":"2025-07-01T19:14:49.201863Z","shell.execute_reply.started":"2025-07-01T19:14:40.193162Z","shell.execute_reply":"2025-07-01T19:14:49.201275Z"}},"outputs":[{"name":"stderr","text":"2025-07-01 19:14:45.340615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751397285.365814     357 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751397285.373615     357 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Load data\ndf = pd.read_parquet('/kaggle/input/your-dataset-name/data.parquet')\ndf_sample = df.sample(10000, random_state=42)  # Use subset for efficiency\n\n# Create label column\nsource_to_label = {'Human': 0}\nai_sources = [\n    'Bloom-7B', 'Claude-Instant-v1', 'Claude-v1', 'Cohere-Command',\n    'Dolphin-2.5-Mixtral-8x7B', 'Dolphin-Mixtral-8x7B', 'Falcon-180B',\n    'Flan-T5-Base', 'Flan-T5-Large', 'Flan-T5-Small', 'Flan-T5-XL', 'Flan-T5-XXL',\n    'GLM-130B', 'GPT-3.5', 'GPT-4', 'GPT-J', 'GPT-NeoX', 'Gemini-Pro',\n    'Goliath-120B', 'LLaMA-13B', 'LLaMA-2-70B', 'LLaMA-2-7B', 'LLaMA-30B',\n    'LLaMA-65B', 'LLaMA-7B', 'LZLV-70B', 'Mistral-7B', 'Mistral-7B-OpenOrca',\n    'Mixtral-8x7B', 'MythoMax-L2-13B', 'Neural-Chat-7B', 'Noromaid-20B',\n    'Nous-Capybara-34B', 'Nous-Capybara-7B', 'Nous-Hermes-LLaMA-2-13B',\n    'Nous-Hermes-LLaMA-2-70B', 'OPT-1.3B', 'OPT-125M', 'OPT-13B', 'OPT-2.7B',\n    'OPT-30B', 'OPT-350M', 'OPT-6.7B', 'OpenChat-3.5', 'OpenHermes-2-Mistral-7B',\n    'OpenHermes-2.5-Mistral-7B', 'PaLM-2', 'Psyfighter-13B', 'Psyfighter-2-13B',\n    'RWKV-5-World-3B', 'StripedHyena-Nous-7B', 'T0-11B', 'T0-3B', 'Text-Ada-001',\n    'Text-Babbage-001', 'Text-Curie-001', 'Text-Davinci-001', 'Text-Davinci-002',\n    'Text-Davinci-003', 'Toppy-M-7B', 'Unknown', 'YI-34B'\n]\nfor source in ai_sources:\n    source_to_label[source] = 1\n\ndf_sample['label'] = df_sample['source'].map(source_to_label).fillna(1)\nprint(\"Label distribution:\")\nprint(df_sample['label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:14:49.203341Z","iopub.execute_input":"2025-07-01T19:14:49.204094Z","iopub.status.idle":"2025-07-01T19:15:06.157547Z","shell.execute_reply.started":"2025-07-01T19:14:49.204074Z","shell.execute_reply":"2025-07-01T19:15:06.156868Z"}},"outputs":[{"name":"stdout","text":"Label distribution:\nlabel\n1    5607\n0    4393\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df_sample['prompt'] = df_sample.apply(\n    lambda row: (\n        \"Classify the following text as AI-generated or Human-generated. \"\n        \"Reply in this format:\\n\"\n        \"Answer - <AI-generated or Human-generated>\\n\"\n        \"Reasoning - <step-by-step explanation>\\n\\n\"\n        f\"Text: {row['text']}\\n\"\n        f\"Answer - {'Human-generated' if row['label']==0 else 'AI-generated'}\\n\"\n        f\"Reasoning - <your reasoning here>\"\n    ),\n    axis=1\n)\nprint(\"\\nFirst 3 prompts:\")\nprint(df_sample[['text', 'label', 'prompt']].head(3))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:15:06.158240Z","iopub.execute_input":"2025-07-01T19:15:06.158438Z","iopub.status.idle":"2025-07-01T19:15:06.274688Z","shell.execute_reply.started":"2025-07-01T19:15:06.158414Z","shell.execute_reply":"2025-07-01T19:15:06.273974Z"}},"outputs":[{"name":"stdout","text":"\nFirst 3 prompts:\n                                                     text  label  \\\n705752  Cognitive enhancement drugs, also known as noo...      1   \n222201  The First Aid: Main Steps and Action Plan Cour...      0   \n270777  Contemplation of Indifference in Elie Wieselâ€™s...      0   \n\n                                                   prompt  \n705752  Classify the following text as AI-generated or...  \n222201  Classify the following text as AI-generated or...  \n270777  Classify the following text as AI-generated or...  \n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Tokenize in batches\nbatch_size = 1000\ninput_ids = []\nattention_mask = []\n\nfor i in tqdm(range(0, len(df_sample), batch_size)):\n    batch = df_sample['prompt'].iloc[i:i+batch_size].tolist()\n    inputs_batch = tokenizer(batch, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n    input_ids.append(inputs_batch['input_ids'])\n    attention_mask.append(inputs_batch['attention_mask'])\n\nprint(\"Dataset tokenized in batches!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:15:06.275394Z","iopub.execute_input":"2025-07-01T19:15:06.275671Z","iopub.status.idle":"2025-07-01T19:15:15.422177Z","shell.execute_reply.started":"2025-07-01T19:15:06.275642Z","shell.execute_reply":"2025-07-01T19:15:15.421325Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92fbbc85fb1e44d89e0c5b0467a676a7"}},"metadata":{}},{"name":"stdout","text":"Dataset tokenized in batches!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"microsoft/phi-2\",\n    quantization_config=bnb_config,\n    trust_remote_code=True,\n    device_map=\"auto\"\n)\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=[\"Wqkv\", \"fc1\", \"fc2\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\"\n)\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:15:15.424199Z","iopub.execute_input":"2025-07-01T19:15:15.424715Z","iopub.status.idle":"2025-07-01T19:15:18.848125Z","shell.execute_reply.started":"2025-07-01T19:15:15.424694Z","shell.execute_reply":"2025-07-01T19:15:18.847447Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c00f085cf53546ba8600686c4c3d1052"}},"metadata":{}},{"name":"stdout","text":"trainable params: 6,553,600 || all params: 2,786,237,440 || trainable%: 0.2352\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class TokenizedDataset(Dataset):\n    def __init__(self, input_ids, attention_mask):\n        self.input_ids = input_ids\n        self.attention_mask = attention_mask\n    def __len__(self):\n        return len(self.input_ids)\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.input_ids[idx],\n            \"attention_mask\": self.attention_mask[idx]\n        }\n\ninput_ids = torch.cat(input_ids)\nattention_mask = torch.cat(attention_mask)\ndataset = TokenizedDataset(input_ids, attention_mask)\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n\ntrain_dataloader = DataLoader(\n    dataset,\n    batch_size=2,\n    collate_fn=data_collator,\n    shuffle=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:15:18.848697Z","iopub.execute_input":"2025-07-01T19:15:18.848944Z","iopub.status.idle":"2025-07-01T19:15:18.865755Z","shell.execute_reply.started":"2025-07-01T19:15:18.848925Z","shell.execute_reply":"2025-07-01T19:15:18.865012Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\nmodel.train()\n\nfor epoch in range(1):  # 1 epoch for speed\n    total_loss = 0\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n    for batch in progress_bar:\n        batch = {k: v.to(model.device) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        total_loss += loss.item()\n        progress_bar.set_postfix(loss=loss.item())\n    avg_loss = total_loss / len(train_dataloader)\n    print(f\"\\nEpoch {epoch+1} Average Loss: {avg_loss}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T19:15:18.866859Z","iopub.execute_input":"2025-07-01T19:15:18.867060Z","iopub.status.idle":"2025-07-01T21:27:39.160709Z","shell.execute_reply.started":"2025-07-01T19:15:18.867043Z","shell.execute_reply":"2025-07-01T21:27:39.159802Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 1:   0%|          | 0/5000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0ceb19eb3874fa198675583f9f8ef62"}},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1 Average Loss: 1.5296617711246014\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/phi2-text-detector\")\ntokenizer.save_pretrained(\"/kaggle/working/phi2-text-detector\")\nprint(\"Model saved to /kaggle/working/phi2-text-detector\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:27:39.161786Z","iopub.execute_input":"2025-07-01T21:27:39.162054Z","iopub.status.idle":"2025-07-01T21:27:39.579269Z","shell.execute_reply.started":"2025-07-01T21:27:39.162033Z","shell.execute_reply":"2025-07-01T21:27:39.578367Z"}},"outputs":[{"name":"stdout","text":"Model saved to /kaggle/working/phi2-text-detector\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def classify_text(text):\n    few_shot_examples = (\n        \"Classify the following text as AI-generated or Human-generated. \"\n        \"Reply in this format:\\n\"\n        \"Answer - <AI-generated or Human-generated>\\n\"\n        \"Reasoning - <step-by-step explanation>\\n\\n\"\n        \"Text: The quantum superposition principle suggests particles exist in multiple states.\\n\"\n        \"Answer - AI-generated\\n\"\n        \"Reasoning - The text is formal, technical, and lacks personal perspective.\\n\\n\"\n        \"Text: I went to the beach yesterday and found amazing seashells!\\n\"\n        \"Answer - Human-generated\\n\"\n        \"Reasoning - The text contains personal experience and informal language.\\n\\n\"\n        f\"Text: {text}\\n\"\n    )\n    inputs = tokenizer(few_shot_examples, return_tensors=\"pt\", max_length=256, truncation=True).to(model.device)\n    outputs = model.generate(**inputs, max_new_tokens=64)\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    # Simple extraction\n    answer, reasoning = \"Not found\", \"Not found\"\n    for line in response.splitlines():\n        if line.strip().lower().startswith(\"answer\"):\n            answer = line.strip().replace(\"Answer -\", \"\").strip()\n        elif line.strip().lower().startswith(\"reasoning\"):\n            reasoning = line.strip().replace(\"Reasoning -\", \"\").strip()\n    return answer, reasoning, response\n\ntest_text = \"I enjoy gardening on weekends\"\nanswer, reasoning, full_response = classify_text(test_text)\nprint(f\"Answer - {answer}\\nReasoning - {reasoning}\\n\\nFull response:\\n{full_response}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-01T21:29:33.604176Z","iopub.execute_input":"2025-07-01T21:29:33.604883Z","iopub.status.idle":"2025-07-01T21:29:40.950397Z","shell.execute_reply.started":"2025-07-01T21:29:33.604849Z","shell.execute_reply":"2025-07-01T21:29:40.949745Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Answer - Human-generated\nReasoning - The text is\n\nFull response:\nClassify the following text as AI-generated or Human-generated. Reply in this format:\nAnswer - <AI-generated or Human-generated>\nReasoning - <step-by-step explanation>\n\nText: The quantum superposition principle suggests particles exist in multiple states.\nAnswer - AI-generated\nReasoning - The text is formal, technical, and lacks personal perspective.\n\nText: I went to the beach yesterday and found amazing seashells!\nAnswer - Human-generated\nReasoning - The text contains personal experience and informal language.\n\nText: I enjoy gardening on weekends\nAnswer - Human-generated\nReasoning - The text is personal and informal.\n\nText: I love to read books\nAnswer - Human-generated\nReasoning - The text is personal and informal.\n\nText: I love to play soccer\nAnswer - Human-generated\nReasoning - The text is\n","output_type":"stream"}],"execution_count":11}]}